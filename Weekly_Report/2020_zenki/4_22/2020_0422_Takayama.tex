%\documentstyle[epsf,twocolumn]{jarticle}       %LaTeX2.09仕様
\documentclass[twocolumn]{jarticle}     %pLaTeX2e仕様

%\usepackage[backend=bibtex, style=numeric]{biblatex}
%\addbibresource{sankou.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  基本 バージョン
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\topmargin}{-45pt}
%\setlength{\oddsidemargin}{0cm}
\setlength{\oddsidemargin}{-7.5mm}
%\setlength{\evensidemargin}{0cm}
\setlength{\textheight}{24.1cm}
%setlength{\textheight}{25cm}
\setlength{\textwidth}{17.4cm}
%\setlength{\textwidth}{172mm}
\setlength{\columnsep}{11mm}

\setlength{\intextsep}{8pt}
\setlength{\textfloatsep}{8pt}
\setlength{\floatsep}{1pt}

\kanjiskip=.07zw plus.5pt minus.5pt



%【節がかわるごとに(1.1)(1.2) …(2.1)(2.2)と数式番号をつけるとき】
%\makeatletter
%\renewcommand{\theequation}{%
%\thesection.\arabic{equation}} %\@addtoreset{equation}{section}
%\makeatother

%\renewcommand{\arraystretch}{0.95} 行間の設定

\usepackage[dvipdfmx]{graphicx}   %pLaTeX2e仕様(\documentstyle ->\documentclass)
\usepackage{scalefnt}
\usepackage{bm}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[subrefformat=parens]{subcaption}
\captionsetup{compatibility=false}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{comment}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{nidanfloat}
\usepackage[dvipdfmx]{hyperref}

\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\begin{document}

\twocolumn[
\noindent
\hspace{1em}

令和2年4月22日(水) ゼミ資料
\hfill
\ \ B4 高山 裕成

\vspace{2mm}
\hrule
\begin{center}
{\Large  進捗報告}
\end{center}
\hrule
\vspace{3mm}
]

% \footnotesize
\section{今週やったこと}

\begin{itemize}
  \item BERT\cite{BERT} の勉強と動作確認.
  \item optuna の動作確認.
  \item d2v モデルの再構築.
  \item B3 実験のコードの手直し.
\end{itemize}

\section{BERT の勉強と動作確認}
Transformer の構造から入って, 今は BERT のモデルの構造について勉強中.
また, ``transformers" ライブラリ (元 ``pytorch-pretrained-bert") を元に簡単な動作確認を行った.
モデルは日本語 Wikipedia から 全 1,800 万文を用いて事前学習させたモデル\footnote{http://nlp.ist.i.kyoto-u.ac.jp}を利用した.

入力は, 4 コマ漫画データセットのセリフを Juman++ で分かち書きしたものに, ``[CLS]", ``[SEP]" トークンを付加したもの. 出力として, 入力文全体の特徴量を持っている ``[CLS]" トークンに対応する $768$ 次元のベクトルを得た.

以下は自分用のメモである.

\subsection{BERT の特徴}
\subsubsection{文脈に依存した単語ベクトル表現が得られる}
同じ単語であれば, 初めの Embedding によって同じベクトルとして表されるが, 12 段の Transformer を経る内に単語の位置情報ベクトルが変化していくため, 最終的には同じ単語であっても文脈によって異なる単語ベクトル表現が得られる.
\subsubsection{自然言語処理タスクで fine-tuning が容易}
BERT をベースに様々な自然言語処理タスクを行わせるためには, まず事前学習した重みパラメータをを設定し, BERT モデルから出力したテンソルを実施したいタスクに応じてアダプターモジュールを追加し, タスクに応じた出力を得る.

例えば, 2 値分類の感情分析であれば, アダプターモジュールとしては 1 つの全結合層を付加するだけで, 文章の感情分析が可能となる. 学習時にはベースとなる BERT と付加した全結合層の両方を fine-tuning で学習させる.
\subsubsection{Attension によって可視化と解釈が容易}
ディープラーニングにおいて, モデルの出力についての解釈性・説明性が問われる昨今において, Attention による可視化によって我々が推論結果の説明について考えやすい.

\subsection{BERT 次にやること}
\begin{itemize}
  \item BERT の fine-tuning
  \item classification layer として 3層 MLP と Self-Attention を用いる.
  \item 2 つの場合について 4 コマ漫画のセリフの感情推定を行う.
\end{itemize}


\section{d2v モデルの再構築}

B3 実験で用いた Wikipedia, なろう, 青空文庫からなる大規模コーパスは, その大きさながらも良い識別結果が得られなかったので,
一旦 約 10 分の 1 のサイズで Wikipedia のみのコーパスを用いたモデル (model wiki),
前回報告した manga 109 のセリフコーパスを用いたモデル (model manga109),
これら 2 つのコーパスを両方用いたモデル (model wiki + manga109),
の計 3 つの Doc2Vec のモデルを作成した. 学習パラメータは表\ref{tab:d2v}で共通している.

\begin{table}[htb]
\begin{center}
\caption{d2v parameters}
\begin{tabular}{|c|c|}
\hline
parameter & value \\ \hline
vec\_size & 300   \\
epochs    & 30    \\
win\_size & 8     \\
min\_cnt  & 3     \\
alpha     & 0.05  \\
workers   & 4     \\ \hline
\end{tabular}
\label{tab:d2v}
\end{center}
\end{table}

これらのモデルを用いて, B3 実験をやり直して結果をまとめることを課題とする.

\section{B3 実験のコードの手直し}
1 文を単語ごとに分散表現にしてから LSTM に入力する手法の追加. 入力列長は 1 セリフ内の単語数の最大値に合わせ,
それ未満のセリフについては, 前側にゼロパディングする. また, 未知語の場合も, その分散表現をゼロベクトルとする $``<$unk$>"$ トークンに置き換える. (または, 予測値を用いる ?)

\section{課題 優先度準}
\begin{itemize}
  \item BERT の fine-tuning を行う.
  \item 新しく作成した d2v モデルと手法を用いた B3 実験の再実験および結果の比較.
  \item 森先生と大工大の上野先生に 30 話まであるらしい追加データをお願いする.
  \item Data Augmentation の手法の改善案.
\end{itemize}




\bibliographystyle{unsrt}
\bibliography{sankou}
\end{document}
